{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You want to transform this dataset so that it\n",
    "# looks like the following:\n",
    "#\n",
    "#  [CLS] [SENT1_TOK1] ... [SENT1_TOKn] [SEP] [SENT2_TOK1] ... [SENT2_TOKn] [SEP] [SENT3_TOK1]  ... [SENT3_TOKn] [SEP]\n",
    "\n",
    "dataset = [\n",
    "    {\n",
    "        'sentences': [\n",
    "            'This is a sentence.',\n",
    "            'This is another sentence.',\n",
    "            'Together, they make a paragraph.',\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'sentences': [\n",
    "            'This sentence belongs to another sample',\n",
    "            'Overall, the dataset is made of multiple samples.',\n",
    "            'Each sample is made of multiple sentences.',\n",
    "            'Samples might have a different number of sentences.',\n",
    "            'And that is the story!',\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Let's use smashed to do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucas/miniforge3/envs/smashed/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.auto.tokenization_auto import AutoTokenizer\n",
    "from smashed.interfaces.simple import (\n",
    "    TokenizerMapper,\n",
    "    MultiSequenceStriderMapper,\n",
    "    TokensSequencesPaddingMapper,\n",
    "    AttentionMaskSequencePaddingMapper,\n",
    "    SequencesConcatenateMapper,\n",
    "    Python2TorchMapper\n",
    ")\n",
    "from pprint import pprint\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path='bert-base-uncased',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]],\n",
      "  'input_ids': [[2023, 2003, 1037, 6251, 1012],\n",
      "                [2023, 2003, 2178, 6251, 1012],\n",
      "                [2362, 1010, 2027, 2191, 1037, 20423, 1012]]},\n",
      " {'attention_mask': [[1, 1, 1, 1, 1, 1],\n",
      "                     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "                     [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "                     [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "                     [1, 1, 1, 1, 1, 1]],\n",
      "  'input_ids': [[2023, 6251, 7460, 2000, 2178, 7099],\n",
      "                [3452,\n",
      "                 1010,\n",
      "                 1996,\n",
      "                 2951,\n",
      "                 13462,\n",
      "                 2003,\n",
      "                 2081,\n",
      "                 1997,\n",
      "                 3674,\n",
      "                 8168,\n",
      "                 1012],\n",
      "                [2169, 7099, 2003, 2081, 1997, 3674, 11746, 1012],\n",
      "                [8168, 2453, 2031, 1037, 2367, 2193, 1997, 11746, 1012],\n",
      "                [1998, 2008, 2003, 1996, 2466, 999]]}]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# First we need to tokenize each sentence\n",
    "\n",
    "tokenize_mapper = TokenizerMapper(\n",
    "    input_field='sentences',\n",
    "    tokenizer=tokenizer,\n",
    "    add_special_tokens=False,\n",
    "    truncation=True,\n",
    "    max_length=80\n",
    ")\n",
    "\n",
    "tokenized_dataset = tokenize_mapper.map(dataset, remove_columns=True)\n",
    "pprint(tokenized_dataset)\n",
    "pprint(len(tokenized_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]],\n",
      " 'input_ids': [[2023, 2003, 1037, 6251, 1012],\n",
      "               [2023, 2003, 2178, 6251, 1012],\n",
      "               [2362, 1010, 2027, 2191, 1037, 20423, 1012]]}\n"
     ]
    }
   ],
   "source": [
    "# Then we generate new examples so that each contains at most 3 sentences or 512 tokens\n",
    "\n",
    "strider_mapper = MultiSequenceStriderMapper(\n",
    "    max_stride_count=3,\n",
    "    max_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    length_reference_field='input_ids'\n",
    ")\n",
    "\n",
    "strided_dataset = strider_mapper.map(tokenized_dataset)\n",
    "pprint(strided_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [[1, 1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1, 1, 1, 1]],\n",
      " 'input_ids': [[101, 2023, 2003, 1037, 6251, 1012, 102],\n",
      "               [2023, 2003, 2178, 6251, 1012, 102],\n",
      "               [2362, 1010, 2027, 2191, 1037, 20423, 1012, 102]]}\n"
     ]
    }
   ],
   "source": [
    "# we map both the input_ids and the attention mask. Note how we combine \n",
    "# multiple mappers into a pipeline.\n",
    "\n",
    "padding_mappers = TokensSequencesPaddingMapper(\n",
    "    tokenizer=tokenizer,\n",
    "    input_field='input_ids'\n",
    ") >> AttentionMaskSequencePaddingMapper(\n",
    "    tokenizer=tokenizer,\n",
    "    input_field='attention_mask'\n",
    ")\n",
    "\n",
    "padded_dataset = padding_mappers.map(strided_dataset)\n",
    "pprint(padded_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1],\n",
      " 'input_ids': [101,\n",
      "               2023,\n",
      "               2003,\n",
      "               1037,\n",
      "               6251,\n",
      "               1012,\n",
      "               102,\n",
      "               2023,\n",
      "               2003,\n",
      "               2178,\n",
      "               6251,\n",
      "               1012,\n",
      "               102,\n",
      "               2362,\n",
      "               1010,\n",
      "               2027,\n",
      "               2191,\n",
      "               1037,\n",
      "               20423,\n",
      "               1012,\n",
      "               102]}\n"
     ]
    }
   ],
   "source": [
    "# We concatenate all examples in the same stride:\n",
    "concat_mapper = SequencesConcatenateMapper()\n",
    "concatenated_dataset = concat_mapper.map(padded_dataset)\n",
    "pprint(concatenated_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1.], dtype=torch.float16),\n",
      " 'input_ids': tensor([  101,  2023,  2003,  1037,  6251,  1012,   102,  2023,  2003,  2178,\n",
      "         6251,  1012,   102,  2362,  1010,  2027,  2191,  1037, 20423,  1012,\n",
      "          102])}\n"
     ]
    }
   ],
   "source": [
    "# Finally, let us turn all examples into tensors:\n",
    "tensor_mapper = Python2TorchMapper(\n",
    "    field_cast_map={\n",
    "        'input_ids': 'int64',\n",
    "        'attention_mask': 'float16',\n",
    "    }\n",
    ")\n",
    "\n",
    "final_dataset = tensor_mapper.map(concatenated_dataset)\n",
    "pprint(final_dataset[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('smashed')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ecb6e3b32531dbd0c4550ccc6184702d9a712289611c1c0f7b97724e9317547"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
